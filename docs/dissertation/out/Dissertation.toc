\contentsline {chapter}{Abstract}{ii}{Doc-Start}%
\contentsline {chapter}{Acknowledgements}{iii}{Doc-Start}%
\contentsline {chapter}{Declarations}{iv}{Doc-Start}%
\contentsline {chapter}{Abbreviations}{v}{Doc-Start}%
\contentsline {chapter}{List of Figures}{viii}{Doc-Start}%
\contentsline {chapter}{List of Tables}{ix}{Doc-Start}%
\contentsline {chapter}{List of Algorithms}{x}{Doc-Start}%
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Research Overview}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Motivation}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}Contributions}{2}{section.1.3}%
\contentsline {chapter}{\numberline {2}Background}{4}{chapter.2}%
\contentsline {section}{\numberline {2.1}What is the 0/1 Knapsack Problem?}{4}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Problem Definition}{4}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Mathematical Formulation}{4}{subsection.2.1.2}%
\contentsline {section}{\numberline {2.2}What is Dynamic Programming?}{5}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}The State-Transition Equation}{5}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Complexity and Limitations}{6}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Optimizations and Their Inadequacies}{6}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Basics of Reinforcement Learning}{6}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Why Reinforcement Learning?}{7}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}What is Reinforcement Learning?}{7}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}From Bellman to PPO: An Evolutionary Trajectory}{8}{subsection.2.3.3}%
\contentsline {subsubsection}{The Foundational Bellman Equation}{8}{section*.2}%
\contentsline {subsubsection}{Step 1: Bellman to Value Function Approximation}{9}{section*.3}%
\contentsline {subsubsection}{Step 2: Value Approximation to Policy Gradient}{9}{section*.4}%
\contentsline {subsubsection}{Step 3: Monte Carlo Estimation to Actor-Critic (TD)}{9}{section*.5}%
\contentsline {subsubsection}{Step 4: Actor-Critic to Proximal Policy Optimization (PPO)}{10}{section*.6}%
\contentsline {subsubsection}{Summary of the Evolutionary Path}{10}{section*.7}%
\contentsline {chapter}{\numberline {3}Related Work}{11}{chapter.3}%
\contentsline {section}{\numberline {3.1}Constructive Methods}{11}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Vanilla Pointer Networks}{11}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Pointer Networks with Self-Attention}{12}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Hierarchical Pointer Networks}{12}{subsection.3.1.3}%
\contentsline {section}{\numberline {3.2}Improvement Methods}{12}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Local Search Based on Neural Networks}{12}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Heuristics Assisted by Neural Networks}{12}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}The Rise of Proximal Policy Optimization}{12}{section.3.3}%
\contentsline {section}{\numberline {3.4}Positioning the Present Work}{13}{section.3.4}%
\contentsline {chapter}{\numberline {4}Methodology}{14}{chapter.4}%
\contentsline {section}{\numberline {4.1}Problem Formulation as a Sequential Decision Process}{14}{section.4.1}%
\contentsline {section}{\numberline {4.2}Reinforcement Learning Framework}{15}{section.4.2}%
\contentsline {section}{\numberline {4.3}Neural Architecture Design}{15}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Shared Transformer Encoder}{16}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Actor Head: Pointer Network-based Decoder}{16}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Critic Head: MLP for Value Estimation}{17}{subsection.4.3.3}%
\contentsline {section}{\numberline {4.4}Training Techniques}{17}{section.4.4}%
\contentsline {section}{\numberline {4.5}Training Algorithm}{18}{section.4.5}%
\contentsline {chapter}{\numberline {5}Results and Evaluation}{20}{chapter.5}%
\contentsline {section}{\numberline {5.1}Experimental Setup}{20}{section.5.1}%
\contentsline {section}{\numberline {5.2}Results}{20}{section.5.2}%
\contentsline {section}{\numberline {5.3}Evaluation and Discussion}{20}{section.5.3}%
\contentsline {chapter}{\numberline {6}Conclusion and Future Work}{21}{chapter.6}%
\contentsline {section}{\numberline {6.1}Conclusions}{21}{section.6.1}%
\contentsline {section}{\numberline {6.2}Future Work}{21}{section.6.2}%
\contentsline {chapter}{\numberline {A}Software Manual}{23}{chapter.1}%
\contentsline {chapter}{Software Manual}{23}{chapter.1}%
\contentsline {section}{\numberline {A.1}Code Repository}{23}{section.1.1}%
\contentsline {section}{\numberline {A.2}Installation Guide}{23}{section.1.2}%
\contentsline {subsection}{\numberline {A.2.1}System Requirements}{23}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {A.2.2}Dependencies}{23}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {A.2.3}Installation Steps}{24}{subsection.1.2.3}%
\contentsline {section}{\numberline {A.3}File Structure}{24}{section.1.3}%
\contentsline {section}{\numberline {A.4}Running the Code}{24}{section.1.4}%
\contentsline {subsection}{\numberline {A.4.1}Data Generation}{25}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {A.4.2}Model Training}{25}{subsection.1.4.2}%
\contentsline {subsubsection}{Training the Transformer-PPO Model (Primary Contribution)}{25}{section*.11}%
\contentsline {subsubsection}{Training the MLP Model}{25}{section*.12}%
\contentsline {subsubsection}{Training the Pointer Network (REINFORCE / Actor-Critic)}{25}{section*.13}%
\contentsline {subsection}{\numberline {A.4.3}Model Evaluation}{25}{subsection.1.4.3}%
\contentsline {subsubsection}{Dedicated PPO Model Evaluation}{26}{section*.14}%
\contentsline {subsubsection}{Unified Solver Evaluation}{26}{section*.15}%
