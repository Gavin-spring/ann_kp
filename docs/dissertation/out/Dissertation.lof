\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The standard agent-environment interaction loop in Reinforcement Learning. Source: Gymnasium Documentation.}}{8}{figure.caption.1}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces The proposed neural architecture. An input observation is processed by a shared Transformer encoder to produce item-specific contexts and a global pooled feature vector. These are then consumed by a Pointer Network-based actor head and an MLP-based critic head. In our best-performing model, a simple MLP architecture was used for the critic.}}{16}{figure.caption.9}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
