% !TEX root = ../Dissertation.tex

\section{Evaluation and Discussion}
\label{sec:discussion}

The results presented in Section 5.2 confirm the effectiveness of our proposed Transformer-PPO framework. This section provides a deeper analysis of these findings, interpreting the performance differences between the models, discussing the critical role of the underlying architecture and training algorithm, and addressing the limitations and implications of this work.

\subsection{Analysis of Solution Quality (MRE)}
The primary goal of this research was to develop a model capable of generalizing to large-scale problems from training on small ones. Figure~\ref{fig:mre_vs_size} demonstrates that this objective was successfully met.

Our PPO-based model maintains a low and stable Mean Relative Error (MRE) below 30\% for instances up to $n=200$, showcasing high solution quality. This success can be attributed to a synergy of three key factors:
\begin{itemize}
    \item \textbf{Algorithmic Superiority}: The PPO algorithm's Actor-Critic structure, which uses Temporal Difference (TD) updates to estimate advantages, provides a low-variance learning signal. This is substantially more stable than the high-variance Monte Carlo returns used by the baseline Pointer Network's REINFORCE algorithm, which failed to learn an effective policy.
    \item \textbf{Architectural Advantage}: The \textbf{Transformer} encoder is crucial for performance. Its self-attention mechanism processes all items in parallel, capturing the global, combinatorial relationships within the problem instance. This is a more powerful approach than the sequential processing of an LSTM, which was used in the baseline Pointer Network architecture.
    \item \textbf{Framework Robustness}: The use of the \textbf{Stable-Baselines3} framework provides significant benefits, most notably the `VecNormalize` wrapper. The automatic and adaptive normalization of both observations and rewards during training is a key stabilization technique that was absent in the baseline implementations.
\end{itemize}
In contrast, the baseline MLP model completely failed to generalize, confirming that a simple neural network cannot learn the complex, structured policy required for the Knapsack Problem.

\subsection{Analysis of Computational Efficiency (Inference Time)}
While solution quality is paramount, computational efficiency is critical for practical applications.

\subsubsection{Initial Findings and Architectural Overhead}
As shown in the initial time analysis in Figure~\ref{fig:time_vs_size}, our PPO model is orders of magnitude faster than the optimal Gurobi solver but slower than the neural baselines. This overhead is expected and stems from two architectural realities:
\begin{itemize}
    \item The \textbf{Transformer} architecture is inherently more computationally intensive than the lightweight LSTM or MLP models.
    \item The Actor-Critic design requires forward passes through both the policy (actor) and value (critic) networks, adding computational cost compared to an actor-only method like REINFORCE.
\end{itemize}

\subsubsection{The Bottleneck: Iterative vs. Batched Evaluation}
A deeper investigation revealed that the primary bottleneck was not the model's architecture, but the evaluation method. The standard Stable-Baselines3 evaluation loop processes instances iteratively (one by one), which fails to leverage the massive parallel processing power of a GPU.

To demonstrate the model's true inference potential, we developed a custom, proof-of-concept batch solver capable of processing hundreds of instances simultaneously. The preliminary results of this solver are shown in Figure~\ref{fig:batch_solver_results}. The time comparison (Figure~\ref{fig:batch_time}) shows that when properly batched, our PPO model's inference time becomes nearly constant and is significantly faster than all other models. This confirms that the model architecture is highly efficient for parallel execution.

However, as shown in the MRE comparison (Figure~\ref{fig:batch_mre}), the current implementation of this batch solver contains a bug that severely degrades solution quality. While this component is still under development, these preliminary results strongly indicate that the model's perceived slowness is an artifact of the evaluation script, not a fundamental limitation of the model itself.

% \begin{figure}[htbp]
%     \centering
%     \subfloat[MRE vs. Problem Size (Batched)]{
%         \includegraphics[width=0.48\textwidth]{figures/batch_errors.png}
%         \label{fig:batch_mre}
%     }
%     \hfill
%     \subfloat[Inference Time vs. Problem Size (Batched)]{
%         \includegraphics[width=0.48\textwidth]{figures/batch_time.png}
%         \label{fig:batch_time}
%     }
%     \caption{Preliminary results from a custom-built batch evaluation solver. (a) The current implementation shows a degradation in solution quality, which is a work-in-progress. (b) However, it successfully demonstrates that the PPO model's inference time becomes nearly constant and extremely fast when leveraging batch processing.}
%     \label{fig:batch_solver_results}
% \end{figure}

\begin{figure}[htbp]
    \centering
    % Use the subfigure environment for each sub-plot
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/batch_errors.png}
        \caption{MRE vs. Problem Size (Batched)}
        \label{fig:batch_mre}
    \end{subfigure}
    \hfill % This command adds a flexible space between the two subfigures
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/batch_time.png}
        \caption{Inference Time vs. Problem Size (Batched)}
        \label{fig:batch_time}
    \end{subfigure}
    
    % This is the main caption for the entire figure
    \caption{Preliminary results from a custom-built batch evaluation solver. (a) The current implementation shows a degradation in solution quality, which is a work-in-progress. (b) However, it successfully demonstrates that the PPO model's inference time becomes nearly constant and extremely fast when leveraging batch processing.}
    \label{fig:batch_solver_results}
\end{figure}

\subsection{Synthesis and Answering Research Questions}
The experimental results, when properly interpreted, confirm that our framework successfully addresses the core research questions. The model learns a genuinely scalable and generalizable policy for the 0/1 Knapsack Problem, outperforming prior neural methods in solution quality while demonstrating a clear potential for state-of-the-art inference speed. The combination of a Transformer architecture with the PPO algorithm proves to be a robust and effective choice for this class of combinatorial optimization problem.