# configs/config.yaml
# Centralized configuration for the entire project.

# 1. Path Management
# Note: These are relative paths from the project root. The config loader
# will build the absolute paths dynamically at runtime.
paths:
  sources: src
  configs: configs
  data: data
  artifacts: artifacts
  # Subdirectories for artifacts
  models: artifacts/models
  plots: artifacts/plots
  logs: artifacts/logs
  results: artifacts/results
  # Subdirectories for data
  data_training: data/training
  data_testing: data/tiny_testing
  data_validation: data/validation

# 2. TESTING Data Generation Config
data_gen:
  # See src/utils/generator.py for correlation options.
  # e.g., 'uncorrelated', 'weakly_correlated', 'strongly_correlated', 'subset_sum'
  correlation_type: 'uncorrelated'
  # Defines problem sizes for the 0-1 knapsack problem, mainly for TESTING: (start_n, stop_n, step).
  # The number of instances generated per size is defined in generate_data.py.
  n_range: [5, 800, 10] # slightly larger than the DNN training set to test extrapolation
  max_weight: 100
  max_value: 100
  capacity_ratio: 0.5
  capacity_ratio_range: [0.1, 0.9]

# 3. Classic Solvers Config
classic_solvers:
  # List of non-ML solvers to be used by evaluate_solvers.py.
  # Names must match keys in ALGORITHM_REGISTRY in config_loader.py.
  algorithms_to_test:
    - "Gurobi"
    - "2D DP"
    - "1D DP (Optimized)"
    - "Branch and Bound"
    - "Greedy"

# 4. Machine Learning Config
ml:
  training_mode: "RL" # Options: "DNN", "RL"

  # "auto" will be resolved to 'cuda' if available, otherwise 'cpu'.
  device: "auto"

  # The algorithm used as the ground truth for calculating ML model errors.
  baseline_algorithm: "Gurobi"

  approximation_solvers:
    "DNN": "dnn"
    "PointerNet RL": "rl"

  # Settings for generating the ML-specific training/validation datasets.
  # MAX_N, input_size, target_scale are dynamically generated in config loader.
  generation:
    start_n: 5
    end_n: 50 # This is max_n
    step_n: 5
    max_weight: 100
    max_value: 100
    capacity_ratio: 0.5
    capacity_ratio_range: [0.1, 0.9]
    correlation_type: 'uncorrelated'

  dnn:
    # Model architecture and normalization hyperparameters.
    hyperparams:
      max_n_for_architecture: 800 # update to match the max testing size
      max_weight_norm: 100 # same as max_weight in dnn
      max_value_norm: 100 # same as max_value in dnn      
      input_size_factor: 5 # input_size = MAX_N * input_size_factor + input_size_plus
      input_size_plus: 1
      target_scale_factor_multiplier: 1.0 # target_scale = MAX_N * max_value * this_multiplier

    # Training loop settings.
    training:
      total_epochs: 800 # Total number of epochs to train on the entire pooled dataset.
      epochs_per_n: 10 # This is for curriculum learning and is not used by Pooled Data Training
      batch_size: 32
      learning_rate: 0.001
      weight_decay: 0.001

  rl:
    # Model architecture and normalization hyperparameters (ref pemami4911/trainer.py argparse)
    hyperparams:
      embedding_dim: 128
      hidden_dim: 128
      n_glimpses: 1
      tanh_exploration: 10
      use_tanh: true
      max_n: 50 # This is the maximum size of the knapsack problem for RL training.
      
    # Training loop settings.
    training:
      total_epochs: 3
      batch_size: 64
      learning_rate: 0.001
      # For learning rate decay scheduler
      lr_decay_step: 500
      lr_decay_rate: 0.96

      # For gradient clipping
      max_grad_norm: 2.0

      # For the exponential moving average baseline
      baseline_beta: 0.8

    testing:
      batch_size: 32
