# configs/config.yaml
# Centralized configuration for the entire project.

# 1. Path Management
# Note: These are relative paths from the project root. The config loader
# will build the absolute paths dynamically at runtime.
paths:
  sources: src
  configs: configs
  data: data
  artifacts: artifacts
  # Subdirectories for artifacts
  models: artifacts/models
  plots: artifacts/plots
  logs: artifacts/logs
  results: artifacts/results
  # Subdirectories for data
  data_training: data/n50_training
  data_validation: data/n50_validation
  data_testing: data/n50_testing

# 2. TESTING Data Generation Config
data_gen:
  # See src/utils/generator.py for correlation options.
  # e.g., 'uncorrelated', 'weakly_correlated', 'strongly_correlated', 'subset_sum'
  correlation_type: 'uncorrelated'
  # Defines problem sizes for the 0-1 knapsack problem, mainly for TESTING: (start_n, stop_n, step).
  # The number of instances generated per size is defined in generate_data.py.
  n_range: [5, 800, 10] # slightly larger than the DNN training set to test extrapolation
  max_weight: 100
  max_value: 100
  capacity_ratio: 0.5
  capacity_ratio_range: [0.1, 0.9]

# 3. Classic Solvers Config
classic_solvers:
  # List of non-ML solvers to be used by evaluate_solvers.py.
  # Names must match keys in ALGORITHM_REGISTRY in config_loader.py.
  algorithms_to_test:
    - "Gurobi"
    - "2D DP"
    - "1D DP (Optimized)"
    - "Branch and Bound"
    - "Greedy"

# 4. Machine Learning Config
ml:
  training_mode: "RL" # Options: "DNN", "RL"
  device: "auto"  # "auto" will be resolved to 'cuda' if available, otherwise 'cpu'.
  baseline_algorithm: "Gurobi"
  approximation_solvers:
    "DNN": "dnn"
    "PointerNet RL": "rl"

  # Settings for generating the ML-specific training/validation datasets.
  # MAX_N, input_size, target_scale are dynamically generated in config loader.
  generation:
    start_n: 5
    end_n: 50 # This is max_n
    step_n: 5
    max_weight: 100
    max_value: 100
    capacity_ratio: 0.5
    capacity_ratio_range: [0.1, 0.9]
    correlation_type: 'uncorrelated'

  dnn:
    # Model architecture and normalization hyperparameters.
    hyperparams:
      max_n_for_architecture: 800 # update to match the max testing size
      max_weight_norm: 100 # same as max_weight in dnn
      max_value_norm: 100 # same as max_value in dnn      
      input_size_factor: 5 # input_size = MAX_N * input_size_factor + input_size_plus
      input_size_plus: 1
      target_scale_factor_multiplier: 1.0 # target_scale = MAX_N * max_value * this_multiplier

    # Training loop settings.
    training:
      total_epochs: 800 # Total number of epochs to train on the entire pooled dataset.
      epochs_per_n: 10 # This is for curriculum learning and is not used by Pooled Data Training
      batch_size: 32
      learning_rate: 0.001
      weight_decay: 0.001

  rl:
    # ==========================================================
    # 方案一：基于你之前实现的 REINFORCE (Monte Carlo) 版本
    # ==========================================================
    reinforce:
      # --- 模型架构超参数 (旧的LSTM + PointerNet) ---
      hyperparams:
        embedding_dim: 128
        hidden_dim: 128      # LSTM 隐藏层维度
        n_glimpses: 2
        tanh_exploration: 30 # Tanh 探索因子 C
        use_tanh: true       # 是否在注意力中使用tanh

      # --- 训练循环参数 (旧的训练脚本使用) ---
      training:
        total_epochs: 300
        batch_size: 32
        learning_rate: 0.0001
        lr_decay_step: 500
        lr_decay_rate: 0.96
        max_grad_norm: 2.0
        baseline_beta: 0.8     # EMA 基线的beta系数

    # ==========================================================
    # 方案二：基于 gym + stable-baselines3 的 PPO 版本
    # ==========================================================
    ppo:
      # --- 模型架构超参数 (新的Transformer + PointerNet) ---
      hyperparams:
        data_type: "fixed"     # ixed" for fixed-size, "unfixed" for variable size
        max_n: 50              # 训练时最大问题规模
        eval_max_n: 500       # 大于测试时最大问题规模 (用于位置编码)
        item_feature_dim: 2    # 每个物品的特征维度 (weight, value)
        embedding_dim: 128
        n_glimpses: 2          # Actor (PointerDecoder) 的 Glimpse 次数
        nhead: 4               # Encoder (Transformer) 的多头注意力头数
        num_layers: 2          # Encoder (Transformer) 的层数
        n_process_block_iters: 3 # Attention blocks in CriticNetwork

      # --- 训练流程与PPO算法超参数 ---
      training:
        # 训练流程控制
        total_timesteps: 3000000
        eval_freq: 2500
        early_stopping_patience: 10

        # 学习率调度
        learning_rate_initial: 0.00001
        learning_rate_final: 0.000001

        # PPO 核心参数 (与SB3完全对应)
        n_steps: 4096
        batch_size: 64
        n_epochs: 4
        gamma: 0.99
        gae_lambda: 0.95
        clip_range: 0.1
        vf_coef: 0.5
        ent_coef: 0.01
        max_grad_norm: 2.0
      
      # --- 评估流程参数 ---
      testing:
        batch_size: 64
