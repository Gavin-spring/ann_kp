# configs/config.yaml
# Centralized configuration for the entire project.

# 1. Path Management
# Note: These are relative paths from the project root. The config loader
# will build the absolute paths dynamically at runtime.
paths:
  sources: src
  configs: configs
  data: data
  artifacts: artifacts
  # Subdirectories for artifacts
  models: artifacts/models
  plots: artifacts/plots
  logs: artifacts/logs
  results: artifacts/results
  # Subdirectories for data
  data_training: data/training
  data_testing: data/testing
  data_validation: data/validation

# 2. Data Generation Config
data_gen:
  # See src/utils/generator.py for correlation options.
  # e.g., 'uncorrelated', 'weakly_correlated', 'strongly_correlated'
  correlation_type: 'uncorrelated'
  # Defines problem sizes for the 'testing' set: (start_n, stop_n, step).
  # The number of instances generated per size is defined in generate_data.py.
  n_range: [10, 101, 10]
  max_weight: 100
  max_value: 100
  capacity_ratio: 0.5

# 3. Classic Solvers Config
classic_solvers:
  # List of non-ML solvers to be used by evaluate_solvers.py.
  # Names must match keys in ALGORITHM_REGISTRY in config_loader.py.
  algorithms_to_test:
    - "Gurobi"
    - "2D DP"
    - "1D DP (Optimized)"
    - "Branch and Bound"
    - "Greedy"

# 4. Machine Learning Config
ml:
  # "auto" will be resolved to 'cuda' if available, otherwise 'cpu'.
  device: "auto"

  # The algorithm used as the ground truth for calculating ML model errors.
  baseline_algorithm: "Gurobi"
  
  dnn:
    # Settings for generating the ML-specific training/validation datasets.
    generation:
      start_n: 5
      end_n: 100 # match testing data range
      step_n: 5

    # Model architecture and normalization hyperparameters.
    hyperparams:
      max_weight_norm: 100 # same as max_weight in data_gen
      max_value_norm: 100 # same as max_value in data_gen
      # MAX_N is calculated dynamically in the config loader based on generation.end_n.
      input_size_factor: 4 # e.g., input_size = MAX_N * input_size_factor + input_size_plus
      input_size_plus: 1
      target_scale_factor_multiplier: 1.0 # e.g., target_scale = MAX_N * max_value * this_multiplier

    # Training loop settings.
    training:
      # Total number of epochs to train on the entire pooled dataset.
      total_epochs: 150
      # The 'epochs_per_n' parameter is for the old curriculum learning style and is not
      # used by the current DNNSolver.train method, but is kept for reference.
      epochs_per_n: 10
      batch_size: 32
      learning_rate: 0.001
      weight_decay: 0.0001